\section{Design of PCStream}
We describe in detail the proposed automatic stream management technique, {\sf
PCStream}.  We first explain a mechanism that automatically extracts PCs during
run time, and then describe how multiple PCs are assigned to streams which are
delivered to an SSD.  Finally, we introduce `substreams' based on a two-phase
stream assignment technique, which effectively mitigates the effect of a few
ill-behaving PCs with large lifetime variances.

%{\it The PC extractor module}, which is implemented in the ... as part ... of a
%system call handler, computes a PC signature (i.e., a sum of program counter
%values) for each write-related system function. 추가 동작 설명..

Fig.~\ref{fig:architecture} shows an overall organization of {\sf PCStream}.
\textit{The PC extractor module}, which is implemented in the Linux kernel as
part of a system call handler, computes a PC signature (i.e., a sum of program
counter values) for each write-related system function (e.g.,
\texttt{write()}).  This enables us to keep track of PCs corresponding to new
data written by a specific module of an application.  The obtained PC is stored
in an inode data structure of a file system (modified for \textsf{PCStream})
and is delivered to \textit{the lifetime analyzer module} which estimates
expected lifetimes of data belonging to a given PC in the block device level.
In order to efficiently detect the end of data lifetime in append-only
workloads, the lifetime analyzer also intercepts TRIM~\cite{TRIM} requests from
a file system.  Based on the lifetime information, \textit{the PC-to-stream
mapper module} clusters PCs with similar lifetimes and maps them together to
the same stream ID.  This mapping is required because of the limited number of
streams supported by an SSD.

\begin{figure}[t]
	\centering
	\includegraphics[width=0.6\linewidth]{figure/architecture4}
	\vspace{-10pt}
	\caption{An overall architecture of {\sf PCStream}.}
	\label{fig:architecture}
	\vspace{-15pt}
\end{figure}

\vspace{-5pt}
\subsection{Automatic PC computation}
As mentioned earlier, a PC is represented by a PC signature which is defined as
the sum of program counters along the execution path of a function call that
finally reaches a write-related system function. A function call involves
pushing the next program counter, which is used as a return address, to the
stack followed by pushing a frame pointer value.  In general, by using frame
pointer values, we are able to back-track the stack frames of the process and
selectively get return addresses for generating a PC signature.  For example,
Fig.~\ref{fig:getpc}(a) shows the abstracted execution path for flushing data
in RocksDB and Fig.~\ref{fig:getpc}(b) illustrates how a PC signature is obtained
by back-tracking the stack.  
Since a frame pointer value in the stack holds the address of the previous
frame pointer, the PC extractor can easily obtain return addresses and
accumulate them to compute a PC signature. (The return addresses are pushed
before calling the {\sf write()}, {\sf BuildTable()} and {\sf
WriteLevel0Table()} functions.)


%The PC extractor obtains and accumulates each
%return address, which is pushed before calling the {\sf write()}, {\sf
%BuildTable()} and {\sf WriteLevel0Table()} functions, by referring the frame
%pointer which holds the address of the previous frame pointer.

%Unfortunately, C/C++ compilers often optimize an output code so
%that it does not use a frame register if possible.  
The frame pointer-based approach for computing PC signatures, however, is not
always possible because modern C/C++ compilers often do not use the frame
pointer to aggressively minimize the program execution time.
One example is a
{\tt -fomit-frame-pointer} option of GCC~\cite{GCC}. 
%While it is effective in saving
%precious resources like CPU registers, this makes it difficult for us to
%back-track return addresses only. 
Although this option allows the frame pointer to be used as a general-purpose
register for high performance, it makes very difficult for us to back-track
return addresses along the call chains.  

In PCStream, we employ a simple but effective workaround for the frame pointer.
When a write system call is made, we scan the every word in the stack and check
it belongs to the process's code segment.  If the scanned stack word holds a
value within the address range of the code segment, we assume that it is a
return address.  Since scanning the full stack takes too long, we stop the
stack scanning procedure when a sufficient number of return address candidates
are found.  In the current version, we stop when 5 return address candidates
are found.  Although quite ad-hoc, a restricted scan is effective in
distinguishing different PCs because it is very unlikely that two different PCs
follow the same execution path up to five caller functions.   In our evaluation
with a 3.4 GHz CPU machine, the performance overhead of the restricted scan was
almost negligible, taking only 300-400 $n$sec per write system call.

\begin{figure}[b]
	\vspace{-15pt}
	\centering
	\subfloat[An abstracted execution path for flushing data.]{\includegraphics[width=0.45\textwidth]{figure/getpc_1}}  
	\vspace{-10pt}
	\hfill
	\subfloat[with the frame pointer.]{\includegraphics[width=0.245\textwidth]{figure/getpc_2}}
	\subfloat[without the frame pointer.]{\includegraphics[width=0.25\textwidth]{figure/getpc_3}}
	\vspace{-8pt}
	\caption{An example execution path and its PC extraction methods.}
	\label{fig:getpc}
\end{figure}

\vspace{-5pt}
\subsection{PC lifetime prediction}
The prediction of PC lifetimes is rather complicated. 
The data lifetime of the append-only workload is defined 
from when a write request is issued until the TRIM command~\cite{TRIM} is issued to 
the corresponding address.
Before issuing a write request, the lifetime analyzer
maintains the written time and the address as well as the PC signature.
Upon receiving TRIM
commands destined for those LBAs, the lifetime analyzer is able to measure the
exact lifetime of those data. 
Note that, the
same PC may generate multiple data streams with different lifetimes, 
so the average of them is used as PC's expected lifetime.


\vspace{-5pt}
\subsection{Mapping PCs to SSD streams}
Our next step is to map a group of PCs with similar lifetimes to an SSD stream.
This is because of a limited number of stream IDs offered by an SSD. For
example, SSDs used in FStream~\cite{FStream} and AutoStream~\cite{AutoStream}
support only 9 and 16 streams, respectively. To properly group multiple PCs,
the PC-to-stream mapper employs a simple 1-D clustering algorithm.  For all the
PCs, the mapper internally builds a 1-D array sorted by PCs' lifetimes.  Given
an available SSD stream number, it runs the clustering algorithm and determines
the best arrangement of PCs into different classes.  For adapting to changing
workloads, reclustering operations should be regularly performed. Since the
number of PCs created by applications is not limited, the clustering algorithm
must be efficient enough to quickly handle many PCs. Our goal in this work is
to confirm the feasibility of using PCs, so we leave
those issues as our future work.

\vspace{-5pt}
\subsection{Two-phase stream assignment}
\begin{figure}[!t]
\centering
\hspace{2pt}
\subfloat[compaction: all levels]{\includegraphics[width=0.23\textwidth]{figure/pc_3b}}
\subfloat[compaction: L2]{\includegraphics[width=0.23\textwidth]{figure/type_4b}}  % data from 4/03040047
\hfill
\vspace{-10pt}
\subfloat[compaction: L3]{\includegraphics[width=0.23\textwidth]{figure/type_5b}}
\subfloat[compaction: L4] {\includegraphics[width=0.23\textwidth]{figure/type_6b}}
\vspace{-10pt}
\caption{The lifetime distribution of the compaction activity.} 
\label{fig:compaction}
\vspace{-15pt}
\end{figure}

While the PC can be used as a useful indicator that determines the lifetime of
data, we also observe that the same PC could generate data with various
lifetimes. One example is the compaction module of RocksDB. RocksDB maintains
several levels, L1, ..., L$n$, in the persistent storage, except for L0 (or a
memtable) stored in DRAM.  Once one level, say L2, becomes full, all the data
in L2 is compacted to a lower level, i.e., L3.  It involves moving data from L2
to L3, along with the deletion of the old data in L2.  In the
LSM-tree~\cite{LSM}, a higher level is smaller than a lower level (i.e., L2 $<$
L3). Thus, data stored in a higher level is invalidated sooner than those kept
in lower levels, thereby having shorter lifetimes.

%Once the L1 becomes full,
%\textit{all} the data kept in the L1 are moved to the L2 by the compaction
%module.  The same operation is applied to the other levels (i.e., L3, ...,
%L$n-1$).  The compaction involves reading and writing data from a higher level
%(e.g., L1) to a lower level (e.g., L2).  The data in a higher level (e.g., L1)
%is then removed.  

%While the program context can be used as a useful indicator that determines the
%lifetime of data, we also observe that the same PC could generate data 
%with diverged lifetimes. One of the representative examples is the compaction
%module of RocksDB. RocksDB maintains several levels, L1, ..., L$n$, in the
%persistent storage, except for L0 (or a memtable) stored in DRAM.  Data flushed
%from the memtable are first written to the L1.  Once the L1 becomes full,
%\textit{all} the data kept in the L1 are moved to the L2 by the compaction
%module.  The same operation is applied to the other levels (i.e., L3, ...,
%L$n-1$).  The compaction involves reading and writing data from a higher level
%(e.g., L1) to a lower level (e.g., L2).  The data in a higher level (e.g., L1)
%is then removed.  In the LSM-tree, a higher level is smaller than a lower
%level. Thus, data stored in a higher level is invalidated sooner than data kept
%in lower levels, thereby having much shorter lifetimes.

Unfortunately, the compaction is done by the same module for all the levels.
A PC is thus not useful to distinguish data written for different levels.
Fig.~\ref{fig:compaction}(a) shows the lifetimes of automatically identified data written by the
compaction module. 
Figs.~\ref{fig:compaction}(b)-(d) illustrate the lifetimes
of manually identified data.
Data from L1 are likely to have shorter lifetime, while L4 has generally
long-lived data.

In order to automatically assign data from the same PC to different SSD streams
according to their lifetimes, we propose a two-phase method that decides SSD
stream IDs in both a host and an SSD level.  
For such a PC, e.g. compaction PC {\it pID}, {\sf PCStream} roughly assigns a
stream {\it sID} as a guideline.
If there are valid data in a victim block for GC, it means
that those data tend to have longer lifetimes compared to other data even if
they belong to the same stream.
To separate long-lived data of {\it pID}, e.g. L4 data, 
an FTL running inside an SSD allocates a substream for {\it sID}.
The first assignment is for relatively hot data that were invalidated
earlier; the second is for relatively cold ones that are still valid in the
victim.  After that, data with different substream IDs are isolated in
different blocks. 

