\section{Introduction}
\label{sec:intro}
Multi-streamed SSDs provide a special mechanism,
called streams, for a host system to prevent data with different lifetimes 
from being mixed into the same block~\cite{T10, MultiStream}.
When the host system maps two data $D_1$ and $D_2$ to 
different streams $S_1$ and $S_2$, a multi-streamed SSD guarantees that 
$D_1$ and $D_2$ are placed to different blocks.   
Since streams, when properly managed, can be very effective in minimizing 
the copy cost of garbage collection, they
can significantly improve both the performance and lifetime of 
flash-based SSDs~\cite{MultiStream, FStream, AutoStream, Level}.

In order to achieve high performance on multi-streamed SSDs, data with similar 
future update times~\cite{PCHa}
should be allocated 
to the same stream, so that the copy cost of garbage collection can be minimized.
However, since it is difficult to know the future update times {\it a priori} when they are written,
stream allocation decisions are often {\it manually} made for a {\it specific} SSD 
at the application level~\cite{MultiStream} or at the file system level~\cite{FStream}, 
thus making it difficult for multi-streamed SSDs to be widely used in practice.  
In this paper, our long-term goal is to develop a general technique for managing streams 
for {\it any} multi-streamed SSD\footnote{The main difference among different multi-streamed 
SSDs is the maximum number of streams supported.  
For example, XXX has Y streams while ZZZ has A streams.} in a {\it fully-automatic} fashion. 

To the best of our knowledge, AutoStream~\cite{AutoStream} is the only automatic technique 
for managing streams without additional manual work.  
However, since AutoStream predicts data lifetimes using the update frequency 
of the logical block address (LBA), it does not work well with modern append-only workloads 
such as RocksDB~\cite{RocksDB} or Cassandra~\cite{Cassandra}.  
Unlike conventional update workloads where data written to the same LBAs 
often show a strong update locality, 
append-only workloads make it impossible to predict data lifetimes 
from the LBA characteristics (such as access frequency or access patterns).  
For example, as shown in Figure~\ref{fig:lba_lifetime}(b), 
data written to a fixed LBA range over times in RocksDB 
show widely varying data lifetimes, 
thus making it very difficult to allocate streams based on LBA characteristics.

In this paper, we propose a fully automatic stream management technique, called {\sf PCStream}, 
for multi-streamed SSDs based on program contexts (PCs).
Since the key motivation behind {\sf PCStream} was 
that data lifetimes should be estimated at a higher abstraction level than LBAs, 
{\sf PCStream} employed a write program context\footnote{Since we are interested in write-related 
system calls such as write() and writev() in the Linux kernel, 
we call their related program contexts as {\it write program contexts.}}  
as a main unit for managing streams.   
A program context~\cite{PC}, which represents a particular execution phase of a program, 
is known to be an effective hint in separating data with different lifetimes~\cite{PCHa}.  
(That is, the lifetime of data written by the same program context tends to be very similar.)   
PCStream maps an identified program context to a stream.  
Since program contexts can be automatically computed during the run time, 
PCStream does not need any manual work.   
In order to handle append-only workloads as well, 
PCStream extended the definition of the data lifetime to include the TRIM command~\cite{TRIM}.

While {\sf PCStream} works efficiently for most workloads, the lifetime of data written from
a single program context may be quite fluctuating, thus making a pure PC-based approach less effective.   
In {\sf PCStream}, 
when such a PC {\it pID} is observed (which was mapped to a stream {\it sID}, 
we further separate data written by {\it pID}
to a substream {\it subID} of {\it sID} during garbage collection 
so that data with a long lifetime cannot be mixed with data with a short lifetime within the stream {\it sID}.

In order to evaluate the effectiveness of PCStream, 
we have implemented the PCStream at the system call layer
and the block I/O layer in the Linux kernel.
We performed experiments using RocksDB with 
various scenarios.
Our experimental results show that {\sf PCStream} can reduce the
garbage collection overhead as much as manual stream
management techniques while no code modification is necessary.
For a RocksDB benchmark, {\sf PCStream} improved WAF by 40\% over 
the existing automatic technique.

The rest of this paper is organized as follows. 
We explain the key motivations behind {\sf PCStream} in Section 2. 
Section 3 describes 
the design and implementation of {\sf PCStream}.
The experimental results are shown in Section 4. 
Finally, we conclude in Section 5 with a summary and future work. 

