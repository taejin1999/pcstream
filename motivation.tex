\section{Basic Idea}
\subsection{Fallacy: LBA-based lifetime prediction}
Many existing data separation techniques such as~\cite{HotCold, AutoStream} 
estimate the data lifetime based on the update frequency of LBAs.  
For example, AutoStream~\cite{AutoStream} assumes that, if
some LBAs are frequently rewritten by applications, those LBAs hold hot data.
This LBA-based lifetime prediction works poorly on applications, where
a majority of new data are written in an append-only manner.  To understand
the correlation between LBAs and the lifetime of data under append-only
workloads, we analyzed the write pattern of RocksDB~\cite{RocksDB}, which is a
popular key-value store based on the LSM-tree algorithm~\cite{LSM}.

For example, Fig.~\ref{fig:lba_lifetime}(a) shows how LBAs may be related 
to data lifetimes\footnote{The lifetime of data is is defined to be 
the number of write requests between when the data is written 
and when the data is invalidated by an overwrite or a TRIM command.}
in RocksDB~\cite{RocksDB} (which is a popular key value store 
based on the append-only LSB-tree algorithm~\cite{LSM}).  
As shown in Fig.~\ref{fig:lba_lifetime}(a), 
there exists no strong correlation between the LBAs and lifetimes in RocksDB.  
\textcolor{blue}{This is in sharp contrast with a scatter plot for update workloads.
}

We also analyzed 
how the lifetimes of LBAs change under some predictable patterns over times 
although the overall lifetime distribution of LBAs shows large variances.
Fig.~\ref{fig:lba_lifetime}(b) shows a scatter plot of data lifetimes over the logical time 
for a 1-MB chunk with 2048 sectors. 
As shown in Figure~\ref{fig:lba_lifetime}(b), 
for the given chuck, data lifetimes varies in a random fashion.  
\begin{comment}
Over the logical time, the lifetime of data written to the chunk 
varies in an unpredictable fashion.  
For example, at the logical time 10, the lifetime was 1 but it increases about 
2.6 million around the logical time 450 
followed by a rapid drop around the logical time 600. 
\end{comment}
Our investigation strongly suggests that  under append-only
workloads, LBAs couldn't be a useful indicator to decide
the hotness or lifetime of data.

\begin{figure}[t]
	\vspace{-10pt}
	\centering
	\subfloat[Lifetime patterns over LBAs]{\includegraphics[width=0.25\textwidth]{figure/lba_lifetime2}}  % data from 0/03031641
	\subfloat[Lifetime patterns over time]{\includegraphics[width=0.22\textwidth]{figure/lifetime_in_chunk2}}
	%\includegraphics[width=0.9\linewidth]{figure/lba_lifetime} 
	\vspace{-10pt}
	\caption{
		Data lifetime distributions over varying LBAs and logical times.}
		\label{fig:lba_lifetime}
	\vspace{-15pt}
\end{figure}

\vspace{-5pt}
\subsection{Program context as a lifetime predictor}
In developing {\sf PCStream},  our key insight was that in most applications 
(regardless of their I/O workload characteristics), 
1) there are a few dominant I/O activities from an application 
and 2) each dominant I/O activity tends to have unique data lifetime patterns.   
In order to distinguish data by their lifetimes, therefore, 
it is important to effectively distinguish dominant I/O activities from each other.  
For example, in update workloads, 
LBAs alone were effective in separating dominant I/O activities.  

\begin{comment}
In developing PCStream, we started from a simple question: 
how can we extract I/O context from an application? 
For example, in RocksDB, logging, flushing and compaction can be regarded
as different I/O contexts.
RocksDB appends write-ahead logs to storage to ensure data
persistence.  Those logs have short lifetimes because they are quickly deleted
after original data are persistently stored.
The flush module (which materializes the content of a memtable in
DRAM, called an L0 table, to an L1 table in the storage) generates data
with relatively short lifetimes. This is because an L1 table will be flushed to
an L2 table and be removed in the near future. Conversely, a compaction module
writes long-lived data that are unlikely to be removed for a long time.

The above observation implies that, if we are able to know the detailed
behaviors of append-only applications, data with different lifetimes can be
isolated in separate streams in an SSD. As mentioned before, a common
solution~\cite{MultiStream} to realizing this is manually modify an application
code so that each module assigns a unique stream ID to data it generates.
However, owing to considerable implementation efforts required to modify
individual applications, this approach is not widely used in practice.
\end{comment}

In this paper, 
we argue that a program context is a natural and effective solution 
for distinguishing dominant I/O activities.  
Since a PC represents an execution path of an application which invokes 
write-related system functions such as write() and writev() in the Linux kernel,  
we represent the PC by summing program counter values of all the functions 
along the execution path of the PC.   
In RocksDB, for example, dominant I/O activities include logging, 
flushing and compaction.  
Since they are invoked through different function-call paths, 
we can easily identify the dominant I/O activities of RocksDB using PCs. 
Note that using the program context to distinguish data lifetimes is not new.  
Ha et al. proposed a data separation technique based on the program context~\cite{PCHa}.   
However, their work was not designed for append-only applications and didnâ€™t target 
for a multi-stream feature of a modern SSD.

\begin{figure}[!t]
\centering
\vspace{-10pt}
\hspace{1pt}
\subfloat[\texttt{manual}: log]{\includegraphics[width=0.23\textwidth]{figure/type_1b}} % data from 4/03031953 
\subfloat[\texttt{automatic}: log]{\includegraphics[width=0.23\textwidth]{figure/pcID_2b}}
\hfill
\vspace{-10pt}
\subfloat[\texttt{manual}: flush] {\includegraphics[width=0.23\textwidth]{figure/type_3b}}
\subfloat[\texttt{automatic}: flush]{\includegraphics[width=0.23\textwidth]{figure/pcID_3b}}
\caption{Data lifetime distributions of different PCs.} 
\label{fig:types_and_PCs}
\vspace{-15pt}
\end{figure}

In order to validate our hypothesis  that PCs can be used for predicting 
data lifetimes, we
conducted experiments using RocksDB, comparing the accuracy of lifetime
prediction of two different methods: 1) when the type of data streams is
manually identified by modifying the code (\texttt{manual}) and 2) when the
stream type is automatically tagged by PCs (\texttt{automatic}).
Fig.~\ref{fig:types_and_PCs} compares the lifetime distributions of data
separated by \texttt{manual} and \texttt{automatic}.
Figs.~\ref{fig:types_and_PCs}(a) and (b) represent how well the two methods
identify log data written by RocksDB. 
For short-lived log data, the \texttt{automatic} method with PCs
shows high accuracy to detect log data with short lifetimes, which is
comparable to \texttt{manual}. Similarly, Figs.~\ref{fig:types_and_PCs}(c) and
(d) show the accuracy of identifying data written by the flush module of
RocksDB.  As expected, these two graphs have the similar lifetime patterns,
which indicates that PCs are valuable hints for managing data lifetimes.

